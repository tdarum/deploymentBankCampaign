{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imporrt libarary \n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#import google cloud library\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "# from google.cloud import aiplatform\n",
    "# from support_functions import missing_value, fill_missing, list_dtypes\n",
    "\n",
    "## sklearn module\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = bigquery.Client('dtidsUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_job = client.query(f\"\"\"select * from {dataset_id}.{table_id}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'dtidsUS'\n",
    "dataset_id = 'capstone'\n",
    "table_id = 'data_bank_marketing_campaign'\n",
    "region = 'us-central1'\n",
    "bucket_name = 'modul4'\n",
    "blob_name = 'ilham/data_bank_marketing_campaign.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client(project='dtidsus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#uplad to google cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "An exception occurred",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 5\u001b[0m\n\u001b[1;32m      4\u001b[0m data_capstone \u001b[38;5;241m=\u001b[39m bucket\u001b[38;5;241m.\u001b[39mblob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124milham/data_bank_marketing_campaign.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m data_capstone\u001b[38;5;241m.\u001b[39mupload_from_filename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_bank_marketing_campaign.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading model succeeded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/site-packages/google/cloud/storage/blob.py:3003\u001b[0m, in \u001b[0;36mBlob.upload_from_filename\u001b[0;34m(self, filename, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Upload this blob's contents from the content of a named file.\u001b[39;00m\n\u001b[1;32m   2906\u001b[0m \n\u001b[1;32m   2907\u001b[0m \u001b[38;5;124;03mThe content type of the upload will be determined in order\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3000\u001b[0m \u001b[38;5;124;03m    such as delays and deadlines are respected.\u001b[39;00m\n\u001b[1;32m   3001\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3003\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_filename_and_upload(\n\u001b[1;32m   3004\u001b[0m     filename,\n\u001b[1;32m   3005\u001b[0m     content_type\u001b[38;5;241m=\u001b[39mcontent_type,\n\u001b[1;32m   3006\u001b[0m     num_retries\u001b[38;5;241m=\u001b[39mnum_retries,\n\u001b[1;32m   3007\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m   3008\u001b[0m     predefined_acl\u001b[38;5;241m=\u001b[39mpredefined_acl,\n\u001b[1;32m   3009\u001b[0m     if_generation_match\u001b[38;5;241m=\u001b[39mif_generation_match,\n\u001b[1;32m   3010\u001b[0m     if_generation_not_match\u001b[38;5;241m=\u001b[39mif_generation_not_match,\n\u001b[1;32m   3011\u001b[0m     if_metageneration_match\u001b[38;5;241m=\u001b[39mif_metageneration_match,\n\u001b[1;32m   3012\u001b[0m     if_metageneration_not_match\u001b[38;5;241m=\u001b[39mif_metageneration_not_match,\n\u001b[1;32m   3013\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   3014\u001b[0m     checksum\u001b[38;5;241m=\u001b[39mchecksum,\n\u001b[1;32m   3015\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[1;32m   3016\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/site-packages/google/cloud/storage/blob.py:2879\u001b[0m, in \u001b[0;36mBlob._handle_filename_and_upload\u001b[0;34m(self, filename, content_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2877\u001b[0m content_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_content_type(content_type, filename\u001b[38;5;241m=\u001b[39mfilename)\n\u001b[0;32m-> 2879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_obj:\n\u001b[1;32m   2880\u001b[0m     total_bytes \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfstat(file_obj\u001b[38;5;241m.\u001b[39mfileno())\u001b[38;5;241m.\u001b[39mst_size\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_bank_marketing_campaign.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading model succeeded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: An exception occurred"
     ]
    }
   ],
   "source": [
    "try : \n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.get_bucket(bucket_name) # Add bucket name\n",
    "    data_capstone = bucket.blob('ilham/data_bank_marketing_campaign.csv')\n",
    "    data_capstone.upload_from_filename('data_bank_marketing_campaign.csv')\n",
    "\n",
    "    print (\"Uploading model succeeded\")\n",
    "except:\n",
    "    raise TypeError(\"An exception occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client('dtidsUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 POST https://bigquery.googleapis.com/bigquery/v2/projects/dtidsUS/jobs?prettyPrint=false: ProjectId must be non-empty\n\nLocation: None\nJob ID: 1838eedc-be06-45c1-9ed2-a25f9b45f72f\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_job \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mselect * from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/site-packages/google/cloud/bigquery/client.py:3496\u001b[0m, in \u001b[0;36mClient.query\u001b[0;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, api_method)\u001b[0m\n\u001b[1;32m   3485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _job_helpers\u001b[38;5;241m.\u001b[39mquery_jobs_query(\n\u001b[1;32m   3486\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3487\u001b[0m         query,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3493\u001b[0m         job_retry,\n\u001b[1;32m   3494\u001b[0m     )\n\u001b[1;32m   3495\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m api_method \u001b[38;5;241m==\u001b[39m enums\u001b[38;5;241m.\u001b[39mQueryApiMethod\u001b[38;5;241m.\u001b[39mINSERT:\n\u001b[0;32m-> 3496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _job_helpers\u001b[38;5;241m.\u001b[39mquery_jobs_insert(\n\u001b[1;32m   3497\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3498\u001b[0m         query,\n\u001b[1;32m   3499\u001b[0m         job_config,\n\u001b[1;32m   3500\u001b[0m         job_id,\n\u001b[1;32m   3501\u001b[0m         job_id_prefix,\n\u001b[1;32m   3502\u001b[0m         location,\n\u001b[1;32m   3503\u001b[0m         project,\n\u001b[1;32m   3504\u001b[0m         retry,\n\u001b[1;32m   3505\u001b[0m         timeout,\n\u001b[1;32m   3506\u001b[0m         job_retry,\n\u001b[1;32m   3507\u001b[0m     )\n\u001b[1;32m   3508\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unexpected value for api_method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(api_method)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/site-packages/google/cloud/bigquery/_job_helpers.py:159\u001b[0m, in \u001b[0;36mquery_jobs_insert\u001b[0;34m(client, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m query_job\n\u001b[0;32m--> 159\u001b[0m future \u001b[38;5;241m=\u001b[39m do_query()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# The future might be in a failed state now, but if it's\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# unrecoverable, we'll find out when we ask for it's result, at which\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# point, we may retry.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m job_id_given:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/site-packages/google/cloud/bigquery/_job_helpers.py:136\u001b[0m, in \u001b[0;36mquery_jobs_insert.<locals>.do_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m query_job \u001b[38;5;241m=\u001b[39m job\u001b[38;5;241m.\u001b[39mQueryJob(job_ref, query, client\u001b[38;5;241m=\u001b[39mclient, job_config\u001b[38;5;241m=\u001b[39mjob_config)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     query_job\u001b[38;5;241m.\u001b[39m_begin(retry\u001b[38;5;241m=\u001b[39mretry, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core_exceptions\u001b[38;5;241m.\u001b[39mConflict \u001b[38;5;28;01mas\u001b[39;00m create_exc:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# The thought is if someone is providing their own job IDs and they get\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# their job ID generation wrong, this could end up returning results for\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# the wrong query. We thus only try to recover if job ID was not given.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m job_id_given:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py:1383\u001b[0m, in \u001b[0;36mQueryJob._begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"API call:  begin the job via a POST request\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \n\u001b[1;32m   1365\u001b[0m \u001b[38;5;124;03mSee\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;124;03m    ValueError: If the job has already begun.\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1383\u001b[0m     \u001b[38;5;28msuper\u001b[39m(QueryJob, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_begin(client\u001b[38;5;241m=\u001b[39mclient, retry\u001b[38;5;241m=\u001b[39mretry, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mGoogleAPICallError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1385\u001b[0m     exc\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m=\u001b[39m _EXCEPTION_FOOTER_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1386\u001b[0m         message\u001b[38;5;241m=\u001b[39mexc\u001b[38;5;241m.\u001b[39mmessage, location\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation, job_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_id\n\u001b[1;32m   1387\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py:746\u001b[0m, in \u001b[0;36m_AsyncJob._begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;66;03m# jobs.insert is idempotent because we ensure that every new\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# job has an ID.\u001b[39;00m\n\u001b[1;32m    745\u001b[0m span_attributes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: path}\n\u001b[0;32m--> 746\u001b[0m api_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39m_call_api(\n\u001b[1;32m    747\u001b[0m     retry,\n\u001b[1;32m    748\u001b[0m     span_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBigQuery.job.begin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    749\u001b[0m     span_attributes\u001b[38;5;241m=\u001b[39mspan_attributes,\n\u001b[1;32m    750\u001b[0m     job_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    751\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    752\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m    753\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_api_repr(),\n\u001b[1;32m    754\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    755\u001b[0m )\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_properties(api_response)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/site-packages/google/cloud/bigquery/client.py:837\u001b[0m, in \u001b[0;36mClient._call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_span(\n\u001b[1;32m    835\u001b[0m         name\u001b[38;5;241m=\u001b[39mspan_name, attributes\u001b[38;5;241m=\u001b[39mspan_attributes, client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, job_ref\u001b[38;5;241m=\u001b[39mjob_ref\n\u001b[1;32m    836\u001b[0m     ):\n\u001b[0;32m--> 837\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m call()\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[1;32m    294\u001b[0m     target,\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[1;32m    296\u001b[0m     sleep_generator,\n\u001b[1;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[1;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[1;32m    299\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     _retry_error_helper(\n\u001b[1;32m    154\u001b[0m         exc,\n\u001b[1;32m    155\u001b[0m         deadline,\n\u001b[1;32m    156\u001b[0m         sleep,\n\u001b[1;32m    157\u001b[0m         error_list,\n\u001b[1;32m    158\u001b[0m         predicate,\n\u001b[1;32m    159\u001b[0m         on_error,\n\u001b[1;32m    160\u001b[0m         exception_factory,\n\u001b[1;32m    161\u001b[0m         timeout,\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m target()\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/purwadhika/lib/python3.12/site-packages/google/cloud/_http/__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expect_json \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/dtidsUS/jobs?prettyPrint=false: ProjectId must be non-empty\n\nLocation: None\nJob ID: 1838eedc-be06-45c1-9ed2-a25f9b45f72f\n"
     ]
    }
   ],
   "source": [
    "query_job = client.query(f\"\"\"select * from {dataset_id}.{table_id}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_job' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m query_job\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;241m.\u001b[39mto_dataframe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_job' is not defined"
     ]
    }
   ],
   "source": [
    "df = query_job.result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50000</td>\n",
       "      <td>married</td>\n",
       "      <td>Univ</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>131895</td>\n",
       "      <td>132154</td>\n",
       "      <td>129237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50000</td>\n",
       "      <td>married</td>\n",
       "      <td>Univ</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>56626</td>\n",
       "      <td>58512</td>\n",
       "      <td>61926</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40000</td>\n",
       "      <td>married</td>\n",
       "      <td>Univ</td>\n",
       "      <td>male</td>\n",
       "      <td>25</td>\n",
       "      <td>12961</td>\n",
       "      <td>12029</td>\n",
       "      <td>11738</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30000</td>\n",
       "      <td>single</td>\n",
       "      <td>Univ</td>\n",
       "      <td>male</td>\n",
       "      <td>26</td>\n",
       "      <td>28949</td>\n",
       "      <td>30319</td>\n",
       "      <td>29400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30000</td>\n",
       "      <td>married</td>\n",
       "      <td>Univ</td>\n",
       "      <td>male</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL MARRIAGE EDUCATION   SEX  AGE  BILL_AMT1  BILL_AMT2  BILL_AMT3  \\\n",
       "0      50000  married      Univ  male   23     131895     132154     129237   \n",
       "1      50000  married      Univ  male   24      56626      58512      61926   \n",
       "2      40000  married      Univ  male   25      12961      12029      11738   \n",
       "3      30000   single      Univ  male   26      28949      30319      29400   \n",
       "4      30000  married      Univ  male   26          0          0          0   \n",
       "\n",
       "   TARGET  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       0  \n",
       "4       1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#cleansing \n",
    "result = df.drop(['ID'], axis = 1)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIMIT_BAL    0\n",
       "MARRIAGE     4\n",
       "EDUCATION    0\n",
       "SEX          0\n",
       "AGE          4\n",
       "BILL_AMT1    0\n",
       "BILL_AMT2    0\n",
       "BILL_AMT3    0\n",
       "TARGET       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result  = result.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.76      0.72      1980\n",
      "         1.0       0.58      0.48      0.52      1344\n",
      "\n",
      "    accuracy                           0.65      3324\n",
      "   macro avg       0.63      0.62      0.62      3324\n",
      "weighted avg       0.64      0.65      0.64      3324\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cbcbx\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = result.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a dictionary to store label encoders for each column\n",
    "encoders = {}\n",
    "\n",
    "# Iterate through categorical columns\n",
    "for col in categorical_cols:\n",
    "    # Create a LabelEncoder object for each column\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    # Fit and transform the column\n",
    "    result[col] = encoder.fit_transform(result[col])\n",
    "\n",
    "    # Store the encoder in the dictionary for later use\n",
    "    encoders[col] = encoder\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "y = result['TARGET']  \n",
    "X = result.drop(['TARGET'], axis=1)  \n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Save Model\n",
    "model_filename = \"model.pkl\"\n",
    "pickle.dump(model, open(model_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##upload the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading model succeeded\n"
     ]
    }
   ],
   "source": [
    "try : \n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.get_bucket(bucket_name) # Add bucket name\n",
    "    blob_model = bucket.blob('ilham/model/model.pkl')\n",
    "    blob_model.upload_from_filename('model.pkl')\n",
    "\n",
    "    print (\"Uploading model succeeded\")\n",
    "except:\n",
    "    raise TypeError(\"An exception occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"dev_trial.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/41965541199/locations/us-central1/models/4379281146152747008/operations/8350858329844613120\n",
      "Model created. Resource name: projects/41965541199/locations/us-central1/models/4379281146152747008@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/41965541199/locations/us-central1/models/4379281146152747008@1')\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project='dtidsus', location='us-central1')\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name='jaya_model_000',\n",
    "    artifact_uri=\"gs://modul4/ilham/model/\",\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-2:latest\",\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/41965541199/locations/us-central1/endpoints/2241324766407426048/operations/8402649725559373824\n",
      "Endpoint created. Resource name: projects/41965541199/locations/us-central1/endpoints/2241324766407426048\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/41965541199/locations/us-central1/endpoints/2241324766407426048')\n"
     ]
    }
   ],
   "source": [
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=\"jaya-endpoint-000\",\n",
    "    project='dtidsus',\n",
    "    location='us-central1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_replica_count: int = 1\n",
    "max_replica_count: int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying Model projects/41965541199/locations/us-central1/models/4379281146152747008 to Endpoint : projects/41965541199/locations/us-central1/endpoints/2241324766407426048\n",
      "Deploy Endpoint model backing LRO: projects/41965541199/locations/us-central1/endpoints/2241324766407426048/operations/54664866274738176\n",
      "Endpoint model deployed. Resource name: projects/41965541199/locations/us-central1/endpoints/2241324766407426048\n"
     ]
    }
   ],
   "source": [
    "endpoint.deploy( \n",
    "    model=model,\n",
    "    deployed_model_display_name='jaya_model_000',\n",
    "    machine_type='e2-standard-2',\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    sync=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#using endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict your data with online prediction here \n",
    "PROJECT_ID = 'dtidsus'\n",
    "ENDPOINT_ID = \"projects/41965541199/locations/us-central1/endpoints/2241324766407426048\"\n",
    "REGION = 'us-central1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION: Prediction(predictions=[0.0], deployed_model_id='6724688981780332544', metadata=None, model_version_id='1', model_resource_name='projects/41965541199/locations/us-central1/models/4379281146152747008', explanations=None)\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "endpoint = aiplatform.Endpoint(ENDPOINT_ID)\n",
    "prediction = endpoint.predict(instances=[[50000, 1, 3, 1, 23, 131895, 132154, 129237]])\n",
    "\n",
    "print(\"PREDICTION:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#manual file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download model succeeded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob_model = bucket.blob('ilham/model/model.pkl')\n",
    "    blob_model.download_to_filename('model_new.pkl')\n",
    "\n",
    "    print (\"download model succeeded\")\n",
    "except:\n",
    "    raise TypeError(\"An exception occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load = pickle.load(open('model_new.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model_load.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "purwadhika",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
